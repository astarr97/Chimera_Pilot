{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b60104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astar\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "anndata     0.9.2\n",
      "scanpy      1.9.5\n",
      "-----\n",
      "PIL                 9.4.0\n",
      "anyio               NA\n",
      "asciitree           NA\n",
      "asttokens           NA\n",
      "attr                22.1.0\n",
      "babel               2.11.0\n",
      "backcall            0.2.0\n",
      "beta_ufunc          NA\n",
      "binom_ufunc         NA\n",
      "bottleneck          1.3.5\n",
      "brotli              NA\n",
      "certifi             2023.07.22\n",
      "cffi                1.15.1\n",
      "chardet             4.0.0\n",
      "charset_normalizer  2.0.4\n",
      "cloudpickle         2.2.1\n",
      "colorama            0.4.6\n",
      "comm                0.1.2\n",
      "cycler              0.10.0\n",
      "cython_runtime      NA\n",
      "cytoolz             0.12.0\n",
      "dask                2023.6.0\n",
      "dateutil            2.8.2\n",
      "debugpy             1.6.7\n",
      "decorator           5.1.1\n",
      "defusedxml          0.7.1\n",
      "entrypoints         0.4\n",
      "executing           0.8.3\n",
      "fasteners           0.19\n",
      "fastjsonschema      NA\n",
      "h5py                3.7.0\n",
      "hypergeom_ufunc     NA\n",
      "idna                3.4\n",
      "igraph              0.11.4\n",
      "invgauss_ufunc      NA\n",
      "ipykernel           6.19.2\n",
      "ipython_genutils    0.2.0\n",
      "ipywidgets          8.0.4\n",
      "jedi                0.18.1\n",
      "jinja2              3.1.2\n",
      "joblib              1.2.0\n",
      "json5               NA\n",
      "jsonpointer         2.1\n",
      "jsonschema          4.17.3\n",
      "jupyter_server      1.23.4\n",
      "jupyterlab_server   2.22.0\n",
      "kiwisolver          1.4.4\n",
      "leidenalg           0.10.2\n",
      "llvmlite            0.40.0\n",
      "lz4                 4.3.2\n",
      "markupsafe          2.1.1\n",
      "matplotlib          3.7.1\n",
      "mkl                 2.4.0\n",
      "mpl_toolkits        NA\n",
      "msgpack             1.0.3\n",
      "natsort             8.4.0\n",
      "nbformat            5.7.0\n",
      "nbinom_ufunc        NA\n",
      "ncf_ufunc           NA\n",
      "nct_ufunc           NA\n",
      "ncx2_ufunc          NA\n",
      "ntsecuritycon       NA\n",
      "numba               0.57.0\n",
      "numcodecs           0.12.1\n",
      "numexpr             2.8.4\n",
      "numpy               1.24.3\n",
      "packaging           23.0\n",
      "pandas              2.2.1\n",
      "parso               0.8.3\n",
      "patsy               0.5.3\n",
      "pickleshare         0.7.5\n",
      "pkg_resources       NA\n",
      "platformdirs        2.5.2\n",
      "plotly              5.9.0\n",
      "prometheus_client   NA\n",
      "prompt_toolkit      3.0.36\n",
      "psutil              5.9.0\n",
      "pure_eval           0.2.2\n",
      "pvectorc            NA\n",
      "pyarrow             11.0.0\n",
      "pydev_ipython       NA\n",
      "pydevconsole        NA\n",
      "pydevd              2.9.5\n",
      "pydevd_file_utils   NA\n",
      "pydevd_plugins      NA\n",
      "pydevd_tracing      NA\n",
      "pygments            2.15.1\n",
      "pyparsing           3.0.9\n",
      "pyrsistent          NA\n",
      "pythoncom           NA\n",
      "pytz                2022.7\n",
      "pywintypes          NA\n",
      "requests            2.31.0\n",
      "rfc3339_validator   0.1.4\n",
      "rfc3986_validator   0.1.1\n",
      "ruamel              NA\n",
      "scipy               1.10.1\n",
      "seaborn             0.13.2\n",
      "send2trash          NA\n",
      "session_info        1.0.0\n",
      "setuptools          68.0.0\n",
      "six                 1.16.0\n",
      "skewnorm_ufunc      NA\n",
      "sklearn             1.3.0\n",
      "sniffio             1.2.0\n",
      "socks               1.7.1\n",
      "sphinxcontrib       NA\n",
      "stack_data          0.2.0\n",
      "statsmodels         0.14.0\n",
      "tblib               1.7.0\n",
      "terminado           0.17.1\n",
      "texttable           1.7.0\n",
      "threadpoolctl       2.2.0\n",
      "tlz                 0.12.0\n",
      "toolz               0.12.0\n",
      "tornado             6.3.2\n",
      "traitlets           5.7.1\n",
      "typing_extensions   NA\n",
      "urllib3             1.26.16\n",
      "wcwidth             0.2.5\n",
      "websocket           0.58.0\n",
      "win32api            NA\n",
      "win32com            NA\n",
      "win32con            NA\n",
      "win32security       NA\n",
      "win32trace          NA\n",
      "winerror            NA\n",
      "winpty              2.0.10\n",
      "yaml                6.0\n",
      "zarr                2.15.0\n",
      "zipp                NA\n",
      "zmq                 23.2.0\n",
      "zope                NA\n",
      "-----\n",
      "IPython             8.12.0\n",
      "jupyter_client      7.4.9\n",
      "jupyter_core        5.3.0\n",
      "jupyterlab          3.6.3\n",
      "notebook            6.5.4\n",
      "-----\n",
      "Python 3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:38:37) [MSC v.1916 64 bit (AMD64)]\n",
      "Windows-10-10.0.22631-SP0\n",
      "-----\n",
      "Session information updated at 2024-03-16 13:03\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.pyplot import figure\n",
    "from collections import Counter\n",
    "from scipy.stats import spearmanr,pearsonr,fisher_exact,binom_test\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import leidenalg\n",
    "\n",
    "sc.settings.verbosity = 3             # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.logging.print_versions()\n",
    "sc.settings.set_figure_params(dpi=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf07b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do filtering and create pseudobulk CPM\n",
    "\n",
    "#Read in the anndata files created in Demultiplex_cluster_final.ipynb\n",
    "v_brain = sc.read(\"Final/Brain_Subclustered_Prelim.h5ad\")\n",
    "v_mes = sc.read(\"Final/Mesenchymal_Subclustered_Prelim.h5ad\")\n",
    "v_chondro = sc.read(\"Final/Chondrocyte_Subclustered_Prelim.h5ad\")\n",
    "v_inhib = sc.read(\"Final/GABAergic_Forebrain.h5ad\")\n",
    "v = sc.read(\"Final/All_Cells_Start.h5ad\")\n",
    "f = list(pd.read_csv(\"RiboMito_Blacklist.txt\", sep = \"\\t\", header = None)[0]) + list(pd.read_csv(\"Mouse_Sex_Chr_Genes.txt\", sep = \"\\t\", header = None)[0])\n",
    "\n",
    "#Function definitions\n",
    "#Computes counts per 10000\n",
    "def cpm_vec(x):\n",
    "    s = sum(list(x))\n",
    "    x = [i*10000/s for i in list(x)]\n",
    "    return x\n",
    "\n",
    "#Function to filter genes\n",
    "#Rat was a bad choice of variable name since it means ratio here...\n",
    "def remove_genes(df, ft, brain, cut = 0.2, rat = True):\n",
    "    out = []\n",
    "    #If we want to do filtering based on the proportion of cells with at least one count, we set rat = True\n",
    "    #The inputted dataframe needs to already be binarized\n",
    "    if rat:\n",
    "        #Depending on whether it is brain or connective tissue we want different samples\n",
    "        if brain:\n",
    "            #This just works out to the number of cells every time, so proved unnecessary\n",
    "            max_mr1_mi = np.max(df[\"MR1_mi Raw\"])\n",
    "            max_mr1_ri = np.max(df[\"MR1_ri Raw\"])\n",
    "            max_rm1_mi = np.max(df[\"RM1_mi Raw\"])\n",
    "            max_rm1_ri = np.max(df[\"RM1_ri Raw\"])\n",
    "            for index, row in df.iterrows():\n",
    "                #Check if at least one sample has at least cut proportion of cells expressing\n",
    "                if row[\"MR1_mi Raw\"]/max_mr1_mi >= cut or row[\"MR1_ri Raw\"]/max_mr1_ri >= cut or row[\"RM1_mi Raw\"]/max_rm1_mi >= cut or row[\"RM1_ri Raw\"]/max_rm1_ri >= cut:\n",
    "                    out.append(row)\n",
    "        else:\n",
    "            max_mr1_mi = np.max(df[\"MR1_mi Raw\"])\n",
    "            max_mr1_ri = np.max(df[\"MR1_ri Raw\"])\n",
    "            max_rm1_mi = np.max(df[\"RM2_mi Raw\"])\n",
    "            max_rm1_ri = np.max(df[\"RM2_ri Raw\"])\n",
    "            for index, row in df.iterrows():\n",
    "                if row[\"MR1_mi Raw\"]/max_mr1_mi >= cut or row[\"MR1_ri Raw\"]/max_mr1_ri >= cut or row[\"RM2_mi Raw\"]/max_rm1_mi >= cut or row[\"RM2_ri Raw\"]/max_rm1_ri >= cut:\n",
    "                    out.append(row)\n",
    "    else:\n",
    "        #Otherwise, we simply filter based on total counts\n",
    "        for index, row in df.iterrows():\n",
    "            #Require all samples to have greater counts\n",
    "            if ft == \"and\":\n",
    "                if row[\"MR1_mi Raw\"] >= cut and row[\"MR1_ri Raw\"] >= cut and row[\"RM1_mi Raw\"] >= cut and row[\"RM1_ri Raw\"] >= cut and row[\"RM2_mi Raw\"] >= cut and row[\"RM2_ri Raw\"] >= cut:\n",
    "                    out.append(row)\n",
    "            #Require just one sample to have greater counts\n",
    "            elif ft == \"or\":\n",
    "                if row[\"MR1_mi Raw\"] >= cut or row[\"MR1_ri Raw\"] >= cut or row[\"RM1_mi Raw\"] >= cut or row[\"RM1_ri Raw\"] >= cut:\n",
    "                    out.append(row)\n",
    "            elif ft == \"or2\":\n",
    "                if (row[\"MR1_mi Raw\"] >= cut or row[\"MR1_ri Raw\"] >= cut) and (row[\"RM1_mi Raw\"] >= cut or row[\"RM1_ri Raw\"] >= cut):\n",
    "                    out.append(row)\n",
    "    to_ret = pd.DataFrame(out)\n",
    "    to_ret.columns = df.columns\n",
    "    return to_ret\n",
    "\n",
    "#Process a cluster (cell type) for input to filtering\n",
    "def process(cluster, name, keep, ct, filt = f, binar = False, out_remove = True, pseudo = 1):\n",
    "    #Subset the anndata object\n",
    "    c = cluster[cluster.obs[\"BCS\"].isin(keep)]\n",
    "    \n",
    "    #Get a dataframe of the counts and set columns/indices\n",
    "    z = pd.DataFrame(c.X.todense()).T\n",
    "    z.index = c.var.index\n",
    "    z.columns = c.obs.index\n",
    "    \n",
    "    #Add information to the ct list that was passed\n",
    "    ct.append([z.shape[1], i, name])\n",
    "    \n",
    "    #Binarize expression if desired\n",
    "    if binar == True:\n",
    "        z = pd.DataFrame(z.astype(bool).astype(int).sum(axis=1))\n",
    "    else:\n",
    "        z = pd.DataFrame(z.sum(axis=1))\n",
    "    \n",
    "    #Remove cells not in filt\n",
    "    z = z.loc[~z.index.isin(filt)]\n",
    "    z.columns = [name + \" Raw\"]\n",
    "    return z, ct\n",
    "\n",
    "#Downsamples counts according to multinomial distribution\n",
    "def downsample_counts(x, target_total, seed):\n",
    "    #Compute the multinomial probability distribution\n",
    "    prob = np.float64(x)/np.sum(x)\n",
    "    \n",
    "    #Set random seed and sample\n",
    "    np.random.seed(seed)\n",
    "    return np.random.multinomial(n=target_total, pvals = prob, size = 1)[0]\n",
    "\n",
    "c = 0\n",
    "#New plan:\n",
    "#Filter out genes that are not expressed in > 20% of cells in one of the cell types\n",
    "chondro = [0]\n",
    "prog = [1]\n",
    "\n",
    "#Define the cell type groups for mesenchymal clusters\n",
    "mes_1 = [1]\n",
    "mes_2 = [2]\n",
    "mes_0 = [0]\n",
    "mes_4 = [4]\n",
    "mes_5 = [5]\n",
    "mes_prog = [3]\n",
    "\n",
    "#Define cell type groups for brain\n",
    "excit = [0, 5]\n",
    "exc_prog = [4]\n",
    "inh_prog = [2]\n",
    "inter_prog = [10]\n",
    "spinal_excit = [6]\n",
    "spinal_inhib = [7]\n",
    "excit_other = [17]\n",
    "\n",
    "#Most inhibitory clusters do not have enough cells\n",
    "#We required at least 10 cells for all of MR_m, MR_r, RM_m, and RM_r\n",
    "#Do the same for GABAergic neurons from the forebrain\n",
    "inhib_all = [0, 1, 3, 4]\n",
    "inhib_0 = [0]\n",
    "inhib_1 = [1]\n",
    "inhib_3 = [3]\n",
    "inhib_4 = [4]\n",
    "inhib_25 = [2, 5]\n",
    "\n",
    "#Only keep a subset of these according to these dictionaries\n",
    "keeping_mes = {\"Mesenchyme_2\":mes_2, \"Mesenchyme_0\":mes_0, \"Mesenchyme_cycling\":mes_prog}\n",
    "keeping_chondro = {\"Chondrocytes\":chondro}\n",
    "keeping_brain = {\"Glutamatergic_neurons\":excit, \"Glutamatergic_progenitors\":exc_prog, \"GABAergic_progenitors\":inh_prog, \"Intermediate_progenitors\":inter_prog, \"Spinal_glutamatergic_neurons\":spinal_excit, \"Spinal_GABAergic_neurons\":spinal_inhib}\n",
    "keeping_inhib = {\"GABAergic_neurons_all\":inhib_all}\n",
    "\n",
    "#Define lists to iterate through\n",
    "samples = [v_brain, v_mes, v_chondro, v_inhib]\n",
    "to_keep = [keeping_brain, keeping_mes, keeping_chondro, keeping_inhib]\n",
    "\n",
    "#Subset the anndata of raw counts by sample\n",
    "v_MR1 = v[v.obs[\"Sample\"].isin([\"MR1\"])]\n",
    "v_MR1_m = v_MR1[v_MR1.obs[\"Species\"].isin([\"Mouse\"])]\n",
    "v_MR1_r = v_MR1[v_MR1.obs[\"Species\"].isin([\"Rat\"])]\n",
    "\n",
    "v_RM1 = v[v.obs[\"Sample\"].isin([\"RM1\"])]\n",
    "v_RM1_m = v_RM1[v_RM1.obs[\"Species\"].isin([\"Mouse\"])]\n",
    "v_RM1_r = v_RM1[v_RM1.obs[\"Species\"].isin([\"Rat\"])]\n",
    "\n",
    "v_RM2 = v[v.obs[\"Sample\"].isin([\"RM2\"])]\n",
    "v_RM2_m = v_RM2[v_RM2.obs[\"Species\"].isin([\"Mouse\"])]\n",
    "v_RM2_r = v_RM2[v_RM2.obs[\"Species\"].isin([\"Rat\"])]\n",
    "\n",
    "v_WT = v[v.obs[\"Sample\"].isin([\"WT\"])]\n",
    "v_WT_m = v_WT[v_WT.obs[\"Species\"].isin([\"Mouse\"])]\n",
    "v_WT_r = v_WT[v_WT.obs[\"Species\"].isin([\"Rat\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b54d4b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glutamatergic_neurons\n",
      "943\n",
      "Glutamatergic_neurons Worked\n",
      "Glutamatergic_progenitors\n",
      "372\n",
      "Glutamatergic_progenitors Worked\n",
      "GABAergic_progenitors\n",
      "392\n",
      "GABAergic_progenitors Worked\n",
      "Intermediate_progenitors\n",
      "135\n",
      "Intermediate_progenitors Worked\n",
      "Spinal_glutamatergic_neurons\n",
      "302\n",
      "Spinal_glutamatergic_neurons Worked\n",
      "Spinal_GABAergic_neurons\n",
      "283\n",
      "Spinal_GABAergic_neurons Worked\n",
      "Mesenchyme_2\n",
      "387\n",
      "Mesenchyme_2 Worked\n",
      "Mesenchyme_0\n",
      "587\n",
      "Mesenchyme_0 Worked\n",
      "Mesenchyme_cycling\n",
      "371\n",
      "Mesenchyme_cycling Worked\n",
      "Chondrocytes\n",
      "211\n",
      "Chondrocytes Worked\n",
      "GABAergic_neurons_all\n",
      "737\n",
      "GABAergic_neurons_all Worked\n"
     ]
    }
   ],
   "source": [
    "#Reran this cell commenting out the line with filtering in it and changing folder\n",
    "for _ in range(len(samples)):\n",
    "    #Set parameters depending on which position in the list (hardcoded based on list definitions above)\n",
    "    if _ == 0:\n",
    "        organ = \"Brain\"\n",
    "        brain_bool = True\n",
    "    elif _ == 1:\n",
    "        organ = \"Mesenchymal\"\n",
    "        brain_bool = False\n",
    "    elif _ == 2:\n",
    "        organ = \"Chondrocyte\"\n",
    "        brain_bool = False\n",
    "    elif _ == 3:\n",
    "        organ = \"Brain\"\n",
    "        brain_bool = True\n",
    "    v_b = samples[_]\n",
    "    keeping = to_keep[_]\n",
    "    ct = []\n",
    "    c += 1\n",
    "    #Compute across all clusters include raw counts for downstream filtering.\n",
    "    #List of ribosomal genes here: http://ribosome.med.miyazaki-u.ac.jp/rpg.cgi?mode=orglist&org=Mus%20musculus\n",
    "    #Removed these along with the mt- genes\n",
    "    d = {}\n",
    "\n",
    "    for key in keeping.keys():\n",
    "        print(key)\n",
    "        i = keeping[key]\n",
    "        #Get the cell barcodes that correspond to the right cell type\n",
    "        keeping_these_cells = list(v_b.obs[v_b.obs[\"leiden\"].isin([str(x) for x in i])][\"BCS\"])\n",
    "        print(len(keeping_these_cells))\n",
    "        \n",
    "        #For each sample, process the anndata to create a dataframe of binarized counts\n",
    "        MR1_mi = process(v_MR1_m, \"MR1_mi\", keeping_these_cells, ct, f, binar = True)\n",
    "        ct = MR1_mi[1]\n",
    "        MR1_mi = MR1_mi[0]\n",
    "        MR1_ri = process(v_MR1_r, \"MR1_ri\", keeping_these_cells, ct, f, binar = True)\n",
    "        ct = MR1_ri[1]\n",
    "        MR1_ri = MR1_ri[0]\n",
    "        RM1_mi = process(v_RM1_m, \"RM1_mi\", keeping_these_cells, ct, f, binar = True)\n",
    "        ct = RM1_mi[1]\n",
    "        RM1_mi = RM1_mi[0]\n",
    "        RM1_ri = process(v_RM1_r, \"RM1_ri\", keeping_these_cells, ct, f, binar = True)\n",
    "        ct = RM1_ri[1]\n",
    "        RM1_ri = RM1_ri[0]\n",
    "        RM2_mi = process(v_RM2_m, \"RM2_mi\", keeping_these_cells, ct, f, binar = True)\n",
    "        ct = RM2_mi[1]\n",
    "        RM2_mi = RM2_mi[0]\n",
    "        RM2_ri = process(v_RM2_r, \"RM2_ri\", keeping_these_cells, ct, f, binar = True)\n",
    "        ct = RM2_ri[1]\n",
    "        RM2_ri = RM2_ri[0]\n",
    "        WT_mi = process(v_WT_m, \"WT_mi\", keeping_these_cells, ct, f, binar = True)\n",
    "        ct = WT_mi[1]\n",
    "        WT_mi = WT_mi[0]\n",
    "        WT_ri = process(v_WT_r, \"WT_ri\", keeping_these_cells, ct, f, binar = True)\n",
    "        ct = WT_ri[1]\n",
    "        WT_ri = WT_ri[0]\n",
    "        \n",
    "        #Join to create the full data frame\n",
    "        vf = MR1_mi.join([MR1_ri, RM1_mi, RM1_ri, RM2_mi, RM2_ri, WT_mi, WT_ri])\n",
    "        \n",
    "        #Requires 20% of cells for at least one sample and at least 10 cells for one sample\n",
    "        vf = remove_genes(remove_genes(vf, \"or\", brain_bool, 0.2), \"or\", brain_bool, 10, rat = False)\n",
    "        keep_genes = list(vf.index)\n",
    "        \n",
    "        #Reprocess things, this time without binarizing\n",
    "        MR1_mi = process(v_MR1_m, \"MR1_mi\", keeping_these_cells, ct, f, binar = False)\n",
    "        MR1_mi = MR1_mi[0]\n",
    "        MR1_ri = process(v_MR1_r, \"MR1_ri\", keeping_these_cells, ct, f, binar = False)\n",
    "        MR1_ri = MR1_ri[0]\n",
    "        RM1_mi = process(v_RM1_m, \"RM1_mi\", keeping_these_cells, ct, f, binar = False)       \n",
    "        RM1_mi = RM1_mi[0]\n",
    "        RM1_ri = process(v_RM1_r, \"RM1_ri\", keeping_these_cells, ct, f, binar = False)\n",
    "        RM1_ri = RM1_ri[0]\n",
    "        RM2_mi = process(v_RM2_m, \"RM2_mi\", keeping_these_cells, ct, f, binar = False)\n",
    "        RM2_mi = RM2_mi[0]\n",
    "        RM2_ri = process(v_RM2_r, \"RM2_ri\", keeping_these_cells, ct, f, binar = False)\n",
    "        RM2_ri = RM2_ri[0]\n",
    "        WT_mi = process(v_WT_m, \"WT_mi\", keeping_these_cells, ct, f, binar = False)\n",
    "        WT_mi = WT_mi[0]\n",
    "        WT_ri = process(v_WT_r, \"WT_ri\", keeping_these_cells, ct, f, binar = False)\n",
    "        WT_ri = WT_ri[0]\n",
    "        \n",
    "        #Join all together\n",
    "        vf = MR1_mi.join([MR1_ri, RM1_mi, RM1_ri, RM2_mi, RM2_ri, WT_mi, WT_ri])\n",
    "        #Filter to only the passing genes identified above\n",
    "        vf = vf[vf.index.isin(keep_genes)]\n",
    "        \n",
    "        vals = [\"MR1_mi\", \"MR1_ri\", \"RM1_mi\", \"RM1_ri\", \"RM2_mi\", \"RM2_ri\", \"WT_mi\", \"WT_ri\"]\n",
    "        kvals_brain = [\"MR1_mi\", \"MR1_ri\", \"RM1_mi\", \"RM1_ri\"]\n",
    "        kvals_chond = [\"MR1_mi\", \"MR1_ri\", \"RM2_mi\", \"RM2_ri\"]\n",
    "        to_min = []\n",
    "        #Need to specify this as we often see very low counts in WT or something else for some cell types\n",
    "        #Specifies the minimum number of counts across samples/conditions that we will use for downsampling\n",
    "        for val in vals:\n",
    "            if _ == 0 or _ == 3:\n",
    "                if val in kvals_brain:\n",
    "                    to_min.append(np.sum(vf[val + \" Raw\"]))\n",
    "            elif _ == 1 or _ == 2:\n",
    "                if val in kvals_chond:\n",
    "                    to_min.append(np.sum(vf[val + \" Raw\"]))\n",
    "        min_counts = np.min(to_min)\n",
    "        d = {}\n",
    "        \n",
    "        #Across 100 downsamplings, compute the counts per 10,000\n",
    "        for iteration in range(0, 100):\n",
    "            for val in vals:\n",
    "                cur_arr = np.array(vf[val + \" Raw\"].copy())\n",
    "                try:\n",
    "                    new_arr = downsample_counts(cur_arr, min_counts, iteration) + 1\n",
    "                    CPM = np.array(cpm_vec(new_arr))\n",
    "                except:\n",
    "                    CPM = np.repeat(0, len(cur_arr))\n",
    "                \n",
    "                if iteration:\n",
    "                    d[val] = d[val] + CPM\n",
    "                else:\n",
    "                    d[val] = CPM\n",
    "        \n",
    "        #Average the counts per 10,000\n",
    "        for val in d.keys():\n",
    "            vf[val + \" Norm CPM\"] = d[val]/100\n",
    "\n",
    "        #Compute all possible log fold-changes\n",
    "        if _ == 0 or _ == 3:\n",
    "\n",
    "            vf[\"DR/HR\"] = np.log2(vf[\"RM1_ri Norm CPM\"]/vf[\"MR1_ri Norm CPM\"])\n",
    "            vf[\"DM/HM\"] = np.log2(vf[\"MR1_mi Norm CPM\"]/vf[\"RM1_mi Norm CPM\"])\n",
    "            vf[\"DM/DR\"] = np.log2(vf[\"MR1_mi Norm CPM\"]/vf[\"RM1_ri Norm CPM\"])\n",
    "            vf[\"HM/HR\"] = np.log2(vf[\"RM1_mi Norm CPM\"]/vf[\"MR1_ri Norm CPM\"])\n",
    "            vf[\"DM/HR\"] = np.log2(vf[\"MR1_mi Norm CPM\"]/vf[\"MR1_ri Norm CPM\"])\n",
    "            vf[\"HM/DR\"] = np.log2(vf[\"RM1_mi Norm CPM\"]/vf[\"RM1_ri Norm CPM\"])\n",
    "        #For chondrocytes we will use the one with more reads\n",
    "        elif _ == 1 or _ == 2:\n",
    "            vf[\"DR/HR\"] = np.log2(vf[\"RM2_ri Norm CPM\"]/vf[\"MR1_ri Norm CPM\"])\n",
    "            vf[\"DM/HM\"] = np.log2(vf[\"MR1_mi Norm CPM\"]/vf[\"RM2_mi Norm CPM\"])\n",
    "            vf[\"DM/DR\"] = np.log2(vf[\"MR1_mi Norm CPM\"]/vf[\"RM2_ri Norm CPM\"])\n",
    "            vf[\"HM/HR\"] = np.log2(vf[\"RM2_mi Norm CPM\"]/vf[\"MR1_ri Norm CPM\"])\n",
    "            vf[\"DM/HR\"] = np.log2(vf[\"MR1_mi Norm CPM\"]/vf[\"MR1_ri Norm CPM\"])\n",
    "            vf[\"HM/DR\"] = np.log2(vf[\"RM2_mi Norm CPM\"]/vf[\"RM2_ri Norm CPM\"])\n",
    "        print(key, \"Worked\")\n",
    "        #vf.to_csv(\"Final/Filtered_Pseudobulked/\" + organ + \"_Or_2010_New_Leiden_NewNorm_NoFilt_\" + key + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c25599c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4720"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "943 + 737 + 211 + 371 + 587 + 387 + 283 + 302 + 135 + 392 + 372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7327b4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain_Or_2010_New_Leiden_NewNorm_GABAergic_neurons_all.csv\n",
      "3795\n",
      "Brain_Or_2010_New_Leiden_NewNorm_GABAergic_progenitors.csv\n",
      "5295\n",
      "Brain_Or_2010_New_Leiden_NewNorm_Glutamatergic_neurons.csv\n",
      "4098\n",
      "Brain_Or_2010_New_Leiden_NewNorm_Glutamatergic_progenitors.csv\n",
      "5016\n",
      "Brain_Or_2010_New_Leiden_NewNorm_Intermediate_progenitors.csv\n",
      "4851\n",
      "Brain_Or_2010_New_Leiden_NewNorm_Spinal_GABAergic_neurons.csv\n",
      "4368\n",
      "Brain_Or_2010_New_Leiden_NewNorm_Spinal_glutamatergic_neurons.csv\n",
      "4610\n",
      "Chondrocyte_Or_2010_New_Leiden_NewNorm_Chondrocytes.csv\n",
      "5282\n",
      "Mesenchymal_Or_2010_New_Leiden_NewNorm_Mesenchyme_0.csv\n",
      "5222\n",
      "Mesenchymal_Or_2010_New_Leiden_NewNorm_Mesenchyme_2.csv\n",
      "5520\n",
      "Mesenchymal_Or_2010_New_Leiden_NewNorm_Mesenchyme_cycling.csv\n",
      "5015\n"
     ]
    }
   ],
   "source": [
    "#Decomposition into extrinsic, intrinsic, and interaction with our now pseudobulked data\n",
    "for file in os.listdir(\"Final/Filtered_Pseudobulked\"):\n",
    "    if \"NewNorm\" in file:\n",
    "        print(file)\n",
    "        \n",
    "        #Read in the file and define lists we will append things to\n",
    "        v = pd.read_csv(\"Final/Filtered_Pseudobulked/\" + file)\n",
    "        extr_prop = []\n",
    "        intr_prop = []\n",
    "        interaction_prop = []\n",
    "        extr = []\n",
    "        intr = []\n",
    "        interaction = []\n",
    "        for index, row in v.iterrows():\n",
    "            #DM/HM\tDR/HR\tDM/DR\tHM/HR\tWMR M/R\tWRM M/R\n",
    "            \n",
    "            #Compute interaction divergence\n",
    "            N = row[\"DM/HM\"] + row[\"DR/HR\"]\n",
    "            \n",
    "            #Compute extrinsic divergence estimates as discussed in the manuscript\n",
    "            E1 = -row[\"DM/HM\"]\n",
    "            E2 = row[\"DR/HR\"]\n",
    "            \n",
    "            #Compute intrinsic divergence estimates as discussed in the manuscript\n",
    "            I1 = row[\"DM/HR\"]\n",
    "            I2 = row[\"HM/DR\"]\n",
    "            \n",
    "            #Average them, the negative signs are needed to make things match the manuscript in terms of directionality\n",
    "            E = -(E1 + E2)\n",
    "            I = -(I1 + I2)\n",
    "            \n",
    "            #Append the means\n",
    "            extr.append(E/2)\n",
    "            intr.append(I/2)\n",
    "            \n",
    "            #Divide interaction divergence by 2 as discussed in the manuscript\n",
    "            interaction.append(N/2)\n",
    "            extr_prop.append(abs(E)/(abs(N) + abs(E) + abs(I)))\n",
    "            intr_prop.append(abs(I)/(abs(N) + abs(E) + abs(I)))\n",
    "            interaction_prop.append(abs(N)/(abs(N) + abs(E) + abs(I)))\n",
    "            \n",
    "        #Set the columns of the dataframe\n",
    "        v[\"Extrinsic\"] = extr\n",
    "        v[\"Intrinsic\"] = intr\n",
    "        v[\"Interaction\"] = interaction\n",
    "        v[\"Proportion extrinsic\"] = extr_prop\n",
    "        v[\"Proportion intrinsic\"] = intr_prop\n",
    "        v[\"Proportion interaction\"] = interaction_prop\n",
    "        \n",
    "        #Classify genes has having some divergence if they have absolute log fold-change greater than 0.5 for at least one of the four comparisons\n",
    "        #If not, then they do not have divergence and are generally ignored\n",
    "        out_div = []\n",
    "        out_nodiv = []\n",
    "        for index, row in v.iterrows():\n",
    "            if abs(row[\"HM/HR\"]) >= 0.5 or abs(row[\"DM/DR\"]) >= 0.5 or abs(row[\"DM/HR\"]) >= 0.5 or abs(row[\"HM/DR\"]) >= 0.5:\n",
    "                out_div.append(row[\"Unnamed: 0\"])\n",
    "            else:\n",
    "                out_nodiv.append(row[\"Unnamed: 0\"])\n",
    "        print(len(out_div))\n",
    "        v_div = v[v[\"Unnamed: 0\"].isin(out_div)].copy()\n",
    "        v_nodiv = v[v[\"Unnamed: 0\"].isin(out_nodiv)].copy()\n",
    "        v_div.to_csv(\"Final/Div/\" + file.replace(\"New_Leiden\", \"Div_New4\"), index = False)\n",
    "        v_nodiv.to_csv(\"Final/NoDiv/\" + file.replace(\"New_Leiden\", \"NoDiv_New4\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e0b4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get counts of cells for each sample\n",
    "def get_cell_counts(v, out_file):\n",
    "    \n",
    "    #Subset to each sample/condtion\n",
    "    v_MR1 = v[v.obs[\"Sample\"].isin([\"MR1\"])]\n",
    "    v_MR1_m = v_MR1[v_MR1.obs[\"Species\"].isin([\"Mouse\"])]\n",
    "    v_MR1_r = v_MR1[v_MR1.obs[\"Species\"].isin([\"Rat\"])]\n",
    "\n",
    "    v_RM1 = v[v.obs[\"Sample\"].isin([\"RM1\"])]\n",
    "    v_RM1_m = v_RM1[v_RM1.obs[\"Species\"].isin([\"Mouse\"])]\n",
    "    v_RM1_r = v_RM1[v_RM1.obs[\"Species\"].isin([\"Rat\"])]\n",
    "\n",
    "    v_RM2 = v[v.obs[\"Sample\"].isin([\"RM2\"])]\n",
    "    v_RM2_m = v_RM2[v_RM2.obs[\"Species\"].isin([\"Mouse\"])]\n",
    "    v_RM2_r = v_RM2[v_RM2.obs[\"Species\"].isin([\"Rat\"])]\n",
    "\n",
    "    v_WT = v[v.obs[\"Sample\"].isin([\"WT\"])]\n",
    "    v_WT_m = v_WT[v_WT.obs[\"Species\"].isin([\"Mouse\"])]\n",
    "    v_WT_r = v_WT[v_WT.obs[\"Species\"].isin([\"Rat\"])]\n",
    "    \n",
    "    #Create counters of the cell types\n",
    "    MR1_m = Counter(v_MR1_m.obs[\"leiden\"])\n",
    "    MR1_r = Counter(v_MR1_r.obs[\"leiden\"])\n",
    "    RM1_m = Counter(v_RM1_m.obs[\"leiden\"])\n",
    "    RM1_r = Counter(v_RM1_r.obs[\"leiden\"])\n",
    "    RM2_m = Counter(v_RM2_m.obs[\"leiden\"])\n",
    "    RM2_r = Counter(v_RM2_r.obs[\"leiden\"])\n",
    "    WT_m = Counter(v_WT_m.obs[\"leiden\"])\n",
    "    WT_r = Counter(v_WT_r.obs[\"leiden\"])\n",
    "    \n",
    "    #Iterate through all the counters, adding the number of cells\n",
    "    out = []\n",
    "    keys = list(MR1_m.keys())\n",
    "    keys.sort(key = lambda x: int(x))\n",
    "    ta_MR1_m = []\n",
    "    ta_MR1_r = []\n",
    "    ta_RM1_m = []\n",
    "    ta_RM1_r = []\n",
    "    ta_RM2_m = []\n",
    "    ta_RM2_r = []\n",
    "    ta_WT_m = []\n",
    "    ta_WT_r = []\n",
    "    for key in keys:\n",
    "        ta_MR1_m.append(MR1_m[key])\n",
    "        ta_MR1_r.append(MR1_r[key])\n",
    "        ta_RM1_m.append(RM1_m[key])\n",
    "        ta_RM1_r.append(RM1_r[key])\n",
    "        ta_RM2_m.append(RM2_m[key])\n",
    "        ta_RM2_r.append(RM2_r[key])\n",
    "        ta_WT_m.append(WT_m[key])\n",
    "        ta_WT_r.append(WT_r[key])\n",
    "    out.append([\"MR1_m\", \"MR_m\"] + ta_MR1_m)\n",
    "    out.append([\"MR1_r\", \"MR_r\"] + ta_MR1_r)\n",
    "    out.append([\"RM1_m\", \"RM_m\"] + ta_RM1_m)\n",
    "    out.append([\"RM1_r\", \"RM_r\"] + ta_RM1_r)\n",
    "    out.append([\"RM2_m\", \"RM_m\"] + ta_RM2_m)\n",
    "    out.append([\"RM2_r\", \"RM_r\"] + ta_RM2_r)\n",
    "    out.append([\"WT_m\", \"Wt_m\"] + ta_WT_m)\n",
    "    out.append([\"WT_r\", \"WT_r\"] + ta_WT_r)\n",
    "    \n",
    "    #Write out\n",
    "    df = pd.DataFrame(out)\n",
    "    df.columns = [\"Sample\", \"Condition\"] + list(keys)\n",
    "    df.to_csv(\"Final/\" + out_file, sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ca06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute cell counts\n",
    "get_cell_counts(v_inhib, \"Cell_counts_GABAergic_neurons.tsv\")\n",
    "get_cell_counts(v_chondro, \"Cell_counts_chondrocytes.tsv\")\n",
    "get_cell_counts(v_brain, \"Cell_counts_neuronal.tsv\")\n",
    "get_cell_counts(v_mes, \"Cell_counts_mesenchymal.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f82f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
